{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## OS\n",
        "- OS : GooGle Colab Pro (Ubuntu 22.04.2 LTS)\n",
        "- RAM : 51.0 GB\n",
        "- CPU : Intel(R) Xeon(R) CPU @ 2.20GHz\n",
        "- Python 3.10.12"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZr6LEVuucg4"
      },
      "source": [
        "# 시작\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3rC9Z2hQnOYg"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "import bisect\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm import tqdm\n",
        "from supervised.automl import AutoML\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "train = pd.read_csv('/train.csv').drop(columns=['SAMPLE_ID'])\n",
        "test = pd.read_csv('/test.csv').drop(columns=['SAMPLE_ID'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojZWmvf3kLrH"
      },
      "outputs": [],
      "source": [
        "train['ATA'] = pd.to_datetime(train['ATA'])\n",
        "test['ATA'] = pd.to_datetime(test['ATA'])\n",
        "\n",
        "for df in [train, test]:\n",
        "    df['year'] = df['ATA'].dt.year\n",
        "    df['month'] = df['ATA'].dt.month\n",
        "    df['day'] = df['ATA'].dt.day\n",
        "    df['hour'] = df['ATA'].dt.hour\n",
        "    df['minute'] = df['ATA'].dt.minute\n",
        "    df['weekday'] = df['ATA'].dt.weekday"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        },
        "id": "3mqSDCcr2RzE",
        "outputId": "4d96ef52-f2c2-40e9-bf90-d9c75ae8f385"
      },
      "outputs": [],
      "source": [
        "test_ = test\n",
        "test_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7AFtPXhE3wv"
      },
      "outputs": [],
      "source": [
        "train['CI_HOUR'] = pd.to_timedelta(train['CI_HOUR'], unit=\"h\")\n",
        "train['Bert_time'] = train['ATA'] + train['CI_HOUR']\n",
        "\n",
        "# Bert_time을 기준으로 풍향, 기온 정보와 CI_HOUR 매핑\n",
        "bert_time_mapping = train.set_index('Bert_time')[['ARI_CO', 'ARI_PO', 'U_WIND', 'V_WIND', 'BN', 'AIR_TEMPERATURE', 'CI_HOUR', 'ATA']]\n",
        "\n",
        "def get_avg_bert_time(row):\n",
        "    # 조건에 따라 동일한 값을 가진 Bert_time 찾기\n",
        "    matching_rows = bert_time_mapping[\n",
        "        (bert_time_mapping['ARI_CO'] == row['ARI_CO']) &\n",
        "        (bert_time_mapping['ARI_PO'] == row['ARI_PO']) &\n",
        "        (bert_time_mapping['U_WIND'] == row['U_WIND']) &\n",
        "        (bert_time_mapping['V_WIND'] == row['V_WIND']) &\n",
        "        (bert_time_mapping['BN'] == row['BN']) &\n",
        "        (bert_time_mapping['AIR_TEMPERATURE'] == row['AIR_TEMPERATURE'])\n",
        "    ]\n",
        "\n",
        "    # 해당 조건을 만족하는 모든 행의 Bert_time 값을 평균냄\n",
        "    avg_bert_time = None\n",
        "    if not matching_rows.empty:\n",
        "        avg_bert_time = matching_rows.index.mean()\n",
        "\n",
        "    return avg_bert_time\n",
        "\n",
        "# 결측치가 아닌 test_ 데이터 선택\n",
        "valid_conditions = (\n",
        "    (test_['U_WIND'] != 0) & (pd.notna(test_['U_WIND'])) &\n",
        "    (test_['V_WIND'] != 0) & (pd.notna(test_['V_WIND'])) &\n",
        "    (test_['BN'] != 0) & (pd.notna(test_['BN'])) &\n",
        "    (test_['AIR_TEMPERATURE'] != 0) & (pd.notna(test_['AIR_TEMPERATURE']))\n",
        ")\n",
        "\n",
        "# 각 test_ 행에 대해 평균 Bert_time 값을 계산 후, 새로운 컬럼에 할당\n",
        "test_['Avg_Bert_Time'] = test_[valid_conditions].apply(get_avg_bert_time, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyWBwMvzUuGY"
      },
      "outputs": [],
      "source": [
        "test_['target'] = test_['Avg_Bert_Time'] - test_['ATA']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocbIvNAiU4DY",
        "outputId": "e06cb9f9-320d-4e62-f84c-d6ece7b4b6f1"
      },
      "outputs": [],
      "source": [
        "test_['target'] = test_['target'].apply(lambda x: x.total_seconds() / 3600)\n",
        "test_['target']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNDD2m15HZyB"
      },
      "outputs": [],
      "source": [
        "test_.loc[test_['target'] < 0, 'target'] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZH9jR9itioV"
      },
      "outputs": [],
      "source": [
        "# test_.to_csv('/content/drive/MyDrive/Colab Notebooks/2023_dacon_HDAI/test_berttime.csv', index=False)\n",
        "test_ = pd.read_csv('/test_berttime.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 808
        },
        "id": "B8_3APy9cwOZ",
        "outputId": "49b8b22e-1d69-4c19-ce23-0c3b707b1e03"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('/train.csv').drop(columns=['SAMPLE_ID'])\n",
        "\n",
        "train['ATA'] = pd.to_datetime(train['ATA'])\n",
        "\n",
        "for df in [train]:\n",
        "    df['year'] = df['ATA'].dt.year\n",
        "    df['month'] = df['ATA'].dt.month\n",
        "    df['day'] = df['ATA'].dt.day\n",
        "    df['hour'] = df['ATA'].dt.hour\n",
        "    df['minute'] = df['ATA'].dt.minute\n",
        "    df['weekday'] = df['ATA'].dt.weekday\n",
        "\n",
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RaI2XqH1eNCB"
      },
      "outputs": [],
      "source": [
        "train.drop(columns=['ATA'], inplace=True)\n",
        "test_.drop(columns=['ATA'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZUACs82LzQ2"
      },
      "outputs": [],
      "source": [
        "train_ = train[(train['DIST'] != 0)].reset_index(drop=True)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "train_['CI_HOUR'].hist(bins=100)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "np.log1p(train_['CI_HOUR']).hist(bins=100)\n",
        "\n",
        "train_['CI_trans'] = np.log1p(train_['CI_HOUR'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vs1jBQksnfU4"
      },
      "source": [
        "# 파생변수 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZHBWt3CwSto",
        "outputId": "bc8b6758-86b8-4e7f-8bc2-1471e08f0714"
      },
      "outputs": [],
      "source": [
        "# 겹치는 항구명 처리\n",
        "train_['ARI_PO'] = train_['ARI_CO'] + train_['ARI_PO']\n",
        "test_['ARI_PO'] = test_['ARI_CO'] + test_['ARI_PO']\n",
        "\n",
        "len(train_['ARI_PO'].unique()), len(train_['PORT_SIZE'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oX6KN8exUezz"
      },
      "outputs": [],
      "source": [
        "def compute_smoothed_means(df, group_cols, target_col, m, global_mean=None):\n",
        "    if global_mean is None:\n",
        "        global_mean = df[target_col].mean()\n",
        "\n",
        "    grouped_mean = df.groupby(group_cols)[target_col].mean()\n",
        "    grouped_count = df.groupby(group_cols)[target_col].count()\n",
        "\n",
        "    smoothed_means = (grouped_mean * grouped_count + global_mean * m) / (grouped_count + m)\n",
        "    return smoothed_means\n",
        "\n",
        "def compute_smoothed_median(df, group_cols, target_col, m, global_median=None):\n",
        "    if global_median is None:\n",
        "        global_median = df[target_col].median()\n",
        "\n",
        "    grouped_median = df.groupby(group_cols)[target_col].median()\n",
        "    grouped_count = df.groupby(group_cols)[target_col].count()\n",
        "\n",
        "    smoothed_medians = (grouped_median * grouped_count + global_median * m) / (grouped_count + m)\n",
        "    return smoothed_medians\n",
        "\n",
        "\n",
        "global_mean = train_['CI_trans'].mean()\n",
        "global_median = train_['CI_trans'].median()\n",
        "\n",
        "smoothed_means = compute_smoothed_means(train_, ['ARI_CO', 'ARI_PO'], 'CI_trans', m=100)\n",
        "train_['CO_PO_mean'] = train_.set_index(['ARI_CO', 'ARI_PO']).index.map(smoothed_means)\n",
        "test_['CO_PO_mean'] = test_.set_index(['ARI_CO', 'ARI_PO']).index.map(smoothed_means)\n",
        "\n",
        "\n",
        "smoothed_mean_ship_type = compute_smoothed_means(train_, ['SHIP_TYPE_CATEGORY'], 'CI_trans', m=100)\n",
        "\n",
        "grouped_mean_co = compute_smoothed_means(train_, ['ARI_CO', 'SHIP_TYPE_CATEGORY'], 'CI_trans', m=100)\n",
        "train_['CO_SHIP_mean'] = train_.set_index(['ARI_CO', 'SHIP_TYPE_CATEGORY']).index.map(grouped_mean_co)\n",
        "test_['CO_SHIP_mean'] = test_.set_index(['ARI_CO', 'SHIP_TYPE_CATEGORY']).index.map(grouped_mean_co)\n",
        "train_['CO_SHIP_mean'].fillna(train_['SHIP_TYPE_CATEGORY'].map(smoothed_mean_ship_type), inplace=True)\n",
        "test_['CO_SHIP_mean'].fillna(test_['SHIP_TYPE_CATEGORY'].map(smoothed_mean_ship_type), inplace=True)\n",
        "\n",
        "grouped_mean_po = compute_smoothed_means(train_, ['ARI_PO', 'SHIP_TYPE_CATEGORY'], 'CI_trans', m=100)\n",
        "train_['PO_SHIP_mean'] = train_.set_index(['ARI_PO', 'SHIP_TYPE_CATEGORY']).index.map(grouped_mean_po)\n",
        "test_['PO_SHIP_mean'] = test_.set_index(['ARI_PO', 'SHIP_TYPE_CATEGORY']).index.map(grouped_mean_po)\n",
        "train_['PO_SHIP_mean'].fillna(train_['SHIP_TYPE_CATEGORY'].map(smoothed_mean_ship_type), inplace=True)\n",
        "test_['PO_SHIP_mean'].fillna(test_['SHIP_TYPE_CATEGORY'].map(smoothed_mean_ship_type), inplace=True)\n",
        "\n",
        "\n",
        "smoothed_mean_dist = compute_smoothed_means(train_, ['ARI_PO'], 'DIST', m=100)\n",
        "smoothed_mean_ci_hour = compute_smoothed_means(train_, ['ARI_PO'], 'CI_HOUR', m=100)\n",
        "\n",
        "grouped = pd.DataFrame({'ARI_PO': smoothed_mean_dist.index,\n",
        "                        'smoothed_mean_dist': smoothed_mean_dist.values,\n",
        "                        'smoothed_mean_ci_hour': smoothed_mean_ci_hour.values})\n",
        "grouped['CI_per_dist'] = grouped['smoothed_mean_ci_hour'] / grouped['smoothed_mean_dist']\n",
        "\n",
        "train_ = train_.merge(grouped[['ARI_PO', 'CI_per_dist']], on='ARI_PO', how='left')\n",
        "test_ = test_.merge(grouped[['ARI_PO', 'CI_per_dist']], on='ARI_PO', how='left')\n",
        "\n",
        "\n",
        "grouped_mean_po = compute_smoothed_means(train_, ['SHIP_TYPE_CATEGORY', 'DEPTH'], 'CI_trans', m=100)\n",
        "train_['SHIP_DEPTH_mean'] = train_.set_index(['SHIP_TYPE_CATEGORY', 'DEPTH']).index.map(grouped_mean_po)\n",
        "test_['SHIP_DEPTH_mean'] = test_.set_index(['SHIP_TYPE_CATEGORY', 'DEPTH']).index.map(grouped_mean_po)\n",
        "test_['SHIP_DEPTH_mean'].fillna(global_mean, inplace=True)\n",
        "\n",
        "grouped_median_po = compute_smoothed_median(train_, ['SHIP_TYPE_CATEGORY', 'DEPTH'], 'CI_trans', m=100)\n",
        "train_['SHIP_DEPTH_median'] = train_.set_index(['SHIP_TYPE_CATEGORY', 'DEPTH']).index.map(grouped_median_po)\n",
        "test_['SHIP_DEPTH_median'] = test_.set_index(['SHIP_TYPE_CATEGORY', 'DEPTH']).index.map(grouped_median_po)\n",
        "test_['SHIP_DEPTH_median'].fillna(global_median, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "voeltGaN-Xnn"
      },
      "outputs": [],
      "source": [
        "def map_smoothed_means_to_data(train_df, test_df, group_cols, target_col, m):\n",
        "    smoothed_means = compute_smoothed_means(train_df, group_cols, target_col, m)\n",
        "    train_df['target_encoding1'] = train_df.set_index(group_cols).index.map(smoothed_means).values\n",
        "    test_df['target_encoding1'] = test_df.set_index(group_cols).index.map(smoothed_means).values\n",
        "    return train_df, test_df\n",
        "\n",
        "m = 100  # 예시 값이며, 실제 환경에 따라 적절한 값을 설정해야 합니다.\n",
        "target_col = 'CI_trans'\n",
        "\n",
        "# 첫 번째 그룹화\n",
        "group_cols = ['ARI_CO', 'ARI_PO', 'SHIP_TYPE_CATEGORY', 'DEPTH']\n",
        "train_, test_ = map_smoothed_means_to_data(train_, test_, group_cols, target_col, m)\n",
        "\n",
        "# NaN 확인 후 두 번째 그룹화\n",
        "if test_['target_encoding1'].isna().any():\n",
        "    group_cols = ['ARI_CO', 'ARI_PO', 'SHIP_TYPE_CATEGORY']\n",
        "    train_, test_ = map_smoothed_means_to_data(train_, test_, group_cols, target_col, m)\n",
        "\n",
        "    # NaN 확인 후 세 번째 그룹화\n",
        "    if test_['target_encoding1'].isna().any():\n",
        "        group_cols = ['ARI_CO', 'ARI_PO']\n",
        "        train_, test_ = map_smoothed_means_to_data(train_, test_, group_cols, target_col, m)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cAg4OeBAjJK"
      },
      "outputs": [],
      "source": [
        "def map_smoothed_medians_to_data(train_df, test_df, group_cols, target_col, m):\n",
        "    smoothed_medians = compute_smoothed_median(train_df, group_cols, target_col, m)\n",
        "    train_df['target_encoding2'] = train_df.set_index(group_cols).index.map(smoothed_medians).values\n",
        "    test_df['target_encoding2'] = test_df.set_index(group_cols).index.map(smoothed_medians).values\n",
        "    return train_df, test_df\n",
        "\n",
        "m = 100  # 예시 값이며, 실제 환경에 따라 적절한 값을 설정해야 합니다.\n",
        "target_col = 'CI_trans'\n",
        "\n",
        "# 첫 번째 그룹화\n",
        "group_cols = ['ARI_CO', 'ARI_PO', 'SHIP_TYPE_CATEGORY', 'DEPTH']\n",
        "train_, test_ = map_smoothed_medians_to_data(train_, test_, group_cols, target_col, m)\n",
        "\n",
        "# NaN 확인 후 두 번째 그룹화\n",
        "if test_['target_encoding2'].isna().any():\n",
        "    group_cols = ['ARI_CO', 'ARI_PO', 'SHIP_TYPE_CATEGORY']\n",
        "    train_, test_ = map_smoothed_medians_to_data(train_, test_, group_cols, target_col, m)\n",
        "\n",
        "    # NaN 확인 후 세 번째 그룹화\n",
        "    if test_['target_encoding2'].isna().any():\n",
        "        group_cols = ['ARI_CO', 'ARI_PO']\n",
        "        train_, test_ = map_smoothed_medians_to_data(train_, test_, group_cols, target_col, m)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "NRuURnPvz-eM",
        "outputId": "ad2dd5e1-cc44-468d-f1f2-e32aae69b632"
      },
      "outputs": [],
      "source": [
        "# https://www.cello-square.com/kr-ko/blog/view-29.do\n",
        "\n",
        "def classify_dwt(value):\n",
        "    if value < 40000:\n",
        "        return 0\n",
        "    elif value < 55000:\n",
        "        return 1\n",
        "    elif value < 60000:\n",
        "        return 2\n",
        "    elif value < 100000:\n",
        "        return 3\n",
        "    elif value < 180000:\n",
        "        return 4\n",
        "    elif value < 200000:\n",
        "        return 5\n",
        "    else:\n",
        "        return 6\n",
        "\n",
        "# 데이터프레임 예시: train_\n",
        "train_['DWT_class'] = train_['DEADWEIGHT'].apply(classify_dwt)\n",
        "test_['DWT_class'] = test_['DEADWEIGHT'].apply(classify_dwt)\n",
        "\n",
        "train_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJEe9DEa6FHT",
        "outputId": "06deb014-6edb-48f0-de73-13ae41d0b2b3"
      },
      "outputs": [],
      "source": [
        "columns_to_convert = ['DWT_class', 'weekday']\n",
        "for col in columns_to_convert:\n",
        "    train_[col] = train_[col].astype('object')\n",
        "    test_[col] = test_[col].astype('object')\n",
        "\n",
        "print(train_.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuPt5oVVCKBX"
      },
      "outputs": [],
      "source": [
        "# train 기준 IQR * 3 밖에 있는 데이터를 변수로 나타내줌\n",
        "grouped = train_.groupby('ARI_PO')['DIST']\n",
        "Q1 = grouped.quantile(0.25)\n",
        "Q3 = grouped.quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "lower_bound = Q1 - 3 * IQR\n",
        "upper_bound = Q3 + 3 * IQR\n",
        "\n",
        "is_outlier_train = train_.apply(lambda row: (row['DIST'] < lower_bound[row['ARI_PO']] or\n",
        "                                           row['DIST'] > upper_bound[row['ARI_PO']]), axis=1)\n",
        "\n",
        "is_outlier_test = test_.apply(lambda row: (row['DIST'] < lower_bound[row['ARI_PO']] or\n",
        "                                          row['DIST'] > upper_bound[row['ARI_PO']]), axis=1)\n",
        "\n",
        "train_['CI_trans_outlier'] = is_outlier_train.astype(int)\n",
        "\n",
        "test_['CI_trans_outlier'] = is_outlier_test.astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-JWk7jfr46h"
      },
      "outputs": [],
      "source": [
        "# 여러 변수 정의\n",
        "def feature_engineering(data):\n",
        "    data['Ship_Age_Impact'] = data['BUILT'] **2\n",
        "    data['Dist_to_Weight_Ratio'] = data['DIST'] / data['PORT_SIZE']\n",
        "    data['Size_Ship'] = data['LENGTH'] * data['BREADTH'] * data['DEPTH']\n",
        "    data['Port_Entry_Efficiency'] = data['PORT_SIZE'] / (data['LENGTH'] * data['BREADTH'])\n",
        "    data['Draught_to_Depth_Ratio'] = data['DRAUGHT'] / data['DEPTH']\n",
        "\n",
        "    return data\n",
        "\n",
        "train_ = feature_engineering(train_)\n",
        "test_ = feature_engineering(test_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jll2buocTKA-",
        "outputId": "378e9f1b-d535-4f50-ba0f-39c06bc69f81"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# 범주형 변수 레이블 인코딩\n",
        "categorical_features = ['ARI_CO', 'ARI_PO', 'SHIP_TYPE_CATEGORY', 'DWT_class', 'weekday', 'ID', 'SHIPMANAGER', 'FLAG']\n",
        "encoders = {}\n",
        "\n",
        "for feature in tqdm(categorical_features, desc=\"Encoding features\"):\n",
        "    le = LabelEncoder()\n",
        "    train_[feature] = le.fit_transform(train_[feature].astype(str))\n",
        "    le_classes_set = set(le.classes_)\n",
        "    test_[feature] = test_[feature].map(lambda s: '-1' if s not in le_classes_set else s)\n",
        "    le_classes = le.classes_.tolist()\n",
        "    bisect.insort_left(le_classes, '-1')\n",
        "    le.classes_ = np.array(le_classes)\n",
        "    test_[feature] = le.transform(test_[feature].astype(str))\n",
        "    encoders[feature] = le"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "PlDZ4m98QOnw",
        "outputId": "ca906339-01c0-4375-cc55-43f045e5d972"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "numeric_cols = ['DEADWEIGHT', 'GT', 'LENGTH', 'Ship_Age_Impact', 'Dist_to_Weight_Ratio', 'Size_Ship', 'Port_Entry_Efficiency', 'Draught_to_Depth_Ratio']\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "train_[numeric_cols] = scaler.fit_transform(train_[numeric_cols])\n",
        "test_[numeric_cols] = scaler.transform(test_[numeric_cols])\n",
        "\n",
        "train_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rqS9w3mIeXO"
      },
      "outputs": [],
      "source": [
        "numeric_cols = train_.select_dtypes(include=[np.number])\n",
        "\n",
        "plt.figure(figsize=(30, 30))\n",
        "sns.heatmap(numeric_cols.corr(), annot=True, fmt='.2f')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpsVtGsprAUy"
      },
      "source": [
        "# mljar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCufU-LdrAtD",
        "outputId": "6b41e17d-b316-41e5-b830-7de974219eb5"
      },
      "outputs": [],
      "source": [
        "!pip install mljar-supervised"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvbjFhQysInG"
      },
      "outputs": [],
      "source": [
        "from supervised.automl import AutoML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "FxFqB6LfvRja",
        "outputId": "deda1d17-06fa-4ea3-a941-01b1c0f4245e"
      },
      "outputs": [],
      "source": [
        "train_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "saPUG0p-rcG9"
      },
      "outputs": [],
      "source": [
        "train_x = train_.drop([\"CI_HOUR\", \"CI_trans\", 'U_WIND', 'V_WIND', 'AIR_TEMPERATURE', 'BN'], axis=1)\n",
        "train_y = train_['CI_trans']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Z9O9g-NrOPH"
      },
      "outputs": [],
      "source": [
        "Cross_validation = {\n",
        "    \"validation_type\": \"kfold\",\n",
        "    \"k_folds\": 10,\n",
        "    \"shuffle\": True,\n",
        "    \"random_seed\": 112\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 968
        },
        "id": "SvHBNYNHrLSb",
        "outputId": "7ecbf49f-b815-4960-80ab-3194cd450fc0"
      },
      "outputs": [],
      "source": [
        "automl = AutoML(mode=\"Compete\", algorithms = ['Decision Tree', 'LightGBM', 'Xgboost', 'CatBoost'],\n",
        "                n_jobs = -1, eval_metric='mae', validation_strategy=Cross_validation, ml_task = \"regression\",\n",
        "                total_time_limit=3600,\n",
        "                results_path=\"/model_weight\")\n",
        "automl.fit(train_x, train_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "273cAWpLgAxG"
      },
      "outputs": [],
      "source": [
        "loaded_automl = AutoML(results_path=\"/model_weight\")\n",
        "\n",
        "test_nozerodist = test_[test_['DIST'] != 0].reset_index(drop=True)\n",
        "\n",
        "test_data = test_nozerodist.drop(columns=['U_WIND', 'V_WIND', 'AIR_TEMPERATURE', 'BN', 'Avg_Bert_Time', 'target']) # 모델과 사용하는 컬럼에 따라 수정이 필요\n",
        "\n",
        "pred = loaded_automl.predict_all(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iu8zRx2XTCyl"
      },
      "outputs": [],
      "source": [
        "prediction = np.expm1(pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "oyQNAgbCnG4w",
        "outputId": "c7c15e18-f466-4c58-a8b1-e371776309df"
      },
      "outputs": [],
      "source": [
        "prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_5TyDs6w1oh"
      },
      "outputs": [],
      "source": [
        "submit = pd.read_csv('/sample_submission.csv')\n",
        "\n",
        "new_test = pd.read_csv('/test.csv')\n",
        "\n",
        "test_filtered = new_test[new_test['DIST'] != 0]\n",
        "\n",
        "new_test['predictions'] = None\n",
        "new_test.loc[test_filtered.index, 'predictions'] = prediction['prediction']\n",
        "\n",
        "def generate_predictions(predictions):\n",
        "    for pred in predictions:\n",
        "        yield pred\n",
        "\n",
        "pred_generator = generate_predictions(prediction['prediction'])\n",
        "\n",
        "final_predictions = []\n",
        "\n",
        "for _, row in new_test.iterrows():\n",
        "    if row['DIST'] == 0:\n",
        "        final_predictions.append(0)\n",
        "    else:\n",
        "        final_predictions.append(next(pred_generator))\n",
        "\n",
        "new_test['predictions'] = final_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJKkmv30i5ky"
      },
      "outputs": [],
      "source": [
        "mask = ~test_['target'].isna()\n",
        "new_test.loc[mask, 'predictions'] = test_.loc[mask, 'target']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wh-LvoS5jFHP"
      },
      "outputs": [],
      "source": [
        "new_test.loc[new_test['predictions'] < 0, 'predictions'] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYw40edepucO"
      },
      "outputs": [],
      "source": [
        "submit = pd.read_csv('/sample_submission.csv')\n",
        "\n",
        "submit.iloc[:, 1] = new_test['predictions']\n",
        "\n",
        "submit.to_csv('/dacon_HDAI_mljar_final_test_load.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCswQfFEr7B-"
      },
      "source": [
        "-------------------------------"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
